{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv3D 主要包括三个类：\n",
    "- 参数设置 Config()\n",
    "- 模型 Conv3D(nn.Module)\n",
    "- 运行 Runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import ctypes\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 GPU 的设置\n",
    "use_gpu = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda:2\" if use_gpu else \"cpu\")\n",
    "\n",
    "class Config():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.dim = 30\n",
    "        self.dim1 = 5\n",
    "        self.dim2 = self.dim//self.dim1\n",
    "        \n",
    "        # config of CNN\n",
    "        self.out_channels = 1           # 卷积后的输出图片有 16 个通道，即需要 16 个卷积核；Conv3D中，该值设置为1可能会使效果变好\n",
    "        self.kernel_size = (3, 2, 3)     # (3, 2, 3) for Conv3D, (3,4) for ConvE, 1 for ConvKB\n",
    "                                         # 卷积核一次处理 3 张图片（h,r,t 的 reshape），每个卷积核大小为 2*3\n",
    "        self.drop_prob = 0.1    # 设置为 0.5 就会 69\n",
    "        \n",
    "        # config of training\n",
    "        self.learning_rate = 0.01      # ConvKB 中设为 0.01；改为0.015 Conv3D 就会停在 69 下不去，应该是陷入局部最优了\n",
    "        self.batch_num = 100\n",
    "        self.epoch_num = 300\n",
    "        self.lmbda = 0.1    # ConvKB 中是 0.2\n",
    "        self.opt_method = \"Adagrad\"\n",
    "        \n",
    "        # config of model storage\n",
    "        self.vali_epoch = 1\n",
    "        self.save_epoch = 1000 \n",
    "        self.mode = \"train\"        # \"train\" or \"test\"\n",
    "        self.checkpoint_path = \"./checkpoints\"\n",
    "        \n",
    "        # self.test_file = \"\"\n",
    "        # self.work_threads = 8 \n",
    "        # self.bern = 0 \n",
    "        \n",
    "        # 调用 C++ 封装的库文件 Base.io\n",
    "        self.clib = ctypes.cdll.LoadLibrary(\"./Base.so\")\n",
    "        self.dataset = \"WN18\"    # \"WN18\" or \"WN18RR\" or \"FB15K\" or \"FB15K237\"\n",
    "        self.in_path = \"./\" + self.dataset + \"/\"    # 将数据集路径传递给 Base.io\n",
    "        \n",
    "        # 库函数参数的数据类型，不设置的话服务就会挂掉\n",
    "        # negative sampling\n",
    "        self.clib.sampling.argtypes = [\n",
    "            ctypes.c_void_p,\n",
    "            ctypes.c_void_p,\n",
    "            ctypes.c_void_p,\n",
    "            ctypes.c_void_p,\n",
    "            ctypes.c_int64,\n",
    "            ctypes.c_int64,\n",
    "            ctypes.c_int64,\n",
    "        ]\n",
    "        \n",
    "        # validation dataset\n",
    "        self.clib.getValidHeadBatch.argtypes = [\n",
    "            ctypes.c_void_p,\n",
    "            ctypes.c_void_p,\n",
    "            ctypes.c_void_p,\n",
    "        ]\n",
    "        self.clib.getValidTailBatch.argtypes = [\n",
    "            ctypes.c_void_p,\n",
    "            ctypes.c_void_p,\n",
    "            ctypes.c_void_p,\n",
    "        ]\n",
    "        self.clib.validHead.argtypes = [ctypes.c_void_p]\n",
    "        self.clib.validTail.argtypes = [ctypes.c_void_p]\n",
    "        \n",
    "        # link prediction test dataset\n",
    "        self.clib.getHeadBatch.argtypes = [\n",
    "            ctypes.c_void_p,\n",
    "            ctypes.c_void_p,\n",
    "            ctypes.c_void_p,\n",
    "        ]\n",
    "        self.clib.getTailBatch.argtypes = [\n",
    "            ctypes.c_void_p,\n",
    "            ctypes.c_void_p,\n",
    "            ctypes.c_void_p,\n",
    "        ]\n",
    "        self.clib.testHead.argtypes = [ctypes.c_void_p]\n",
    "        self.clib.testTail.argtypes = [ctypes.c_void_p]\n",
    "        \n",
    "        self.test_file = \"\"\n",
    "        self.clib.setInPath(ctypes.create_string_buffer(self.in_path.encode(), len(self.in_path)*2))\n",
    "        self.clib.setTestFilePath(ctypes.create_string_buffer(self.test_file.encode(), len(self.test_file)*2))\n",
    "        \n",
    "        self.clib.setBern(0)\n",
    "        self.clib.setWorkThreads(8)\n",
    "        self.clib.randReset()\n",
    "        \n",
    "        self.clib.importTrainFiles()\n",
    "        self.clib.importTestFiles()\n",
    "        self.clib.importTypeFiles()\n",
    "        \n",
    "        # 数据集统计信息\n",
    "        self.ent_num = self.clib.getEntityTotal()\n",
    "        self.rel_num = self.clib.getRelationTotal()\n",
    "        self.train_num = self.clib.getTrainTotal()\n",
    "        self.vali_num = self.clib.getValidTotal()\n",
    "        self.test_num = self.clib.getTestTotal()\n",
    "        \n",
    "        self.batch_size = int(self.train_num / self.batch_num)\n",
    "        \n",
    "con = Config()\n",
    "print(con.ent_num)\n",
    "print(con.rel_num)\n",
    "print(con.train_num)\n",
    "print(con.vali_num)\n",
    "print(con.test_num)\n",
    "print(con.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-823b71b262fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 为 CPU 设置用于生成随机数的种子，以使得结果是确定的\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# 使用多个 GPU 的话，为所有的 GPU 设置种子，torch.cuda.manual_seed() 是为当前 GPU 设置种子\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# 为 CPU 设置用于生成随机数的种子，以使得结果是确定的\n",
    "torch.manual_seed(123)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(123)    # 使用多个 GPU 的话，为所有的 GPU 设置种子，torch.cuda.manual_seed() 是为当前 GPU 设置种子\n",
    "\n",
    "class Conv3D(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(Conv3D, self).__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.batch_h = None\n",
    "        self.batch_t = None\n",
    "        self.batch_r = None\n",
    "        self.batch_y = None\n",
    "        \n",
    "        self.ent_embeddings = nn.Embedding(self.config.ent_num, self.config.dim)\n",
    "        self.rel_embeddings = nn.Embedding(self.config.rel_num, self.config.dim)\n",
    "        \n",
    "        #==================================================ConvKB 1D 网络组件===========================================================\n",
    "#         self.bn1 = nn.BatchNorm2d(num_features=1)\n",
    "#         self.conv_layer = nn.Conv2d(in_channels=1, out_channels=self.config.out_channels, kernel_size=(self.config.kernel_size, 3))\n",
    "#         self.bn2 = nn.BatchNorm2d(num_features = self.config.out_channels)    # 卷积之后进行 batch normalization\n",
    "#         self.dropout = nn.Dropout(self.config.drop_prob)\n",
    "#         self.nonlinear = nn.ReLU()    # 也可以尝试 nn.Tanh 等其他激活函数\n",
    "#         self.fc_layer = nn.Linear(in_features=(self.config.dim-self.config.kernel_size+1)*self.config.out_channels, \n",
    "#                                   out_features=1, bias=False)\n",
    "#         self.criterion = nn.Softplus()    # softplus 的公式就是：y = log(1+exp(x))\n",
    "        #==============================================================================================================================\n",
    "        \n",
    "        \n",
    "        \n",
    "        #====================================================ConvE 2D 网络组件==========================================================\n",
    "#         # 网络组件\n",
    "#         self.conv_layer = nn.Conv2d(in_channels=1, out_channels=self.config.out_channels, kernel_size=self.config.kernel_size)\n",
    "#         self.bn1 = nn.BatchNorm2d(1)\n",
    "#         self.bn2 = nn.BatchNorm2d(self.config.out_channels)\n",
    "#         self.bn3 = nn.BatchNorm1d(1)\n",
    "        \n",
    "#         a = self.config.kernel_size[0]\n",
    "#         b = self.config.kernel_size[1]\n",
    "#         self.conv_size = self.config.out_channels*(3*self.config.dim1-a+1)*(self.config.dim2-b+1)\n",
    "#         self.nonlinear = nn.ReLU()\n",
    "#         self.fc_layer = nn.Linear(in_features=self.conv_size, out_features=1)\n",
    "        \n",
    "#         self.input_dropout = nn.Dropout(self.config.drop_prob)    # 原来是 0.2\n",
    "#         self.feature_map_dropout = nn.Dropout2d(self.config.drop_prob)    # 原来是 0.2\n",
    "#         self.hidden_layer_dropout = nn.Dropout(self.config.drop_prob)     # 原来是 0.3\n",
    "        \n",
    "#         self.criterion = nn.Softplus()    # softplus 的公式是 y = log(1+exp(x))\n",
    "        #==============================================================================================================================\n",
    "        \n",
    "        \n",
    "        \n",
    "        #================================================Conv 3D My model网络组件=======================================================\n",
    "        self.bn1 = nn.BatchNorm3d(num_features=1)\n",
    "        self.conv_layer = nn.Conv3d(in_channels=1, out_channels=self.config.out_channels, kernel_size=self.config.kernel_size)\n",
    "        self.bn2 = nn.BatchNorm3d(num_features=self.config.out_channels)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.config.drop_prob)\n",
    "        # self.dropout = nn.Dropout(self.config.drop_prob)\n",
    "        self.nonlinear = nn.ReLU()    # 非线性激活函数\n",
    "        # 卷积层的输出，一般使用ReLu作为激活函数；循环神经网络中的循环层一般为tanh，或者ReLu；\n",
    "        # 全连接层也多用ReLu，只有在神经网络的输出层，使用全连接层来分类的情况下，才会使用softmax\n",
    "        a = self.config.kernel_size[0]\n",
    "        b = self.config.kernel_size[1]\n",
    "        c = self.config.kernel_size[2]\n",
    "        self.conv_size = self.config.out_channels*(3-a+1)*(self.config.dim1-b+1)*(self.config.dim2-c+1)\n",
    "        self.fc_layer = nn.Linear(in_features=self.conv_size, out_features=1)\n",
    "        \n",
    "        self.criterion = nn.Softplus()    # softplus 的公式是 y = log(1+exp(x))\n",
    "        #========================================================================================================================\n",
    "        \n",
    "        \n",
    "        #================================================Conv 3D 二层卷积组件=======================================================\n",
    "#         self.bn1 = nn.BatchNorm3d(num_features=1)\n",
    "#         self.bn2 = nn.BatchNorm3d(num_features=8)\n",
    "#         self.bn3 = nn.BatchNorm3d(num_features=16)\n",
    "        \n",
    "#         self.conv_layer_1 = nn.Conv3d(in_channels=1, out_channels=8, kernel_size=(3,2,3))    # 卷积完了的形状应该是 3-3+1=1，5-2+1=4,6-3+1=4\n",
    "#         self.conv_layer_2 = nn.Conv3d(in_channels=8, out_channels=16, kernel_size=(1,4,4))    # 卷积完了应该是（1,1,1）\n",
    "        \n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "#         self.nonlinear = nn.ReLU()    # 非线性激活函数\n",
    "#         self.fc_layer = nn.Linear(in_features=16, out_features=1)\n",
    "#         self.conv_size = 16\n",
    "        \n",
    "#         self.criterion = nn.Softplus()    # softplus 的公式是 y = log(1+exp(x))\n",
    "        #========================================================================================================================\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 初始化 embeddings\n",
    "        nn.init.xavier_uniform_(self.ent_embeddings.weight.data)\n",
    "        nn.init.xavier_uniform_(self.rel_embeddings.weight.data)\n",
    "        \n",
    "        # 初始化卷积层和全连接层的参数\n",
    "        # nn.init.xavier_uniform_(self.conv_layer.weight.data)\n",
    "        nn.init.xavier_uniform_(self.conv_layer.weight.data)\n",
    "        nn.init.xavier_uniform_(self.fc_layer.weight.data)\n",
    "        \n",
    "    \n",
    "    def cal_score(self):\n",
    "        \n",
    "        #================================================test ConvKB 1D 卷积========================================================\n",
    "#         # 查表得到 embeddings\n",
    "#         h = self.ent_embeddings(self.batch_h)\n",
    "#         t = self.ent_embeddings(self.batch_t)\n",
    "#         r = self.rel_embeddings(self.batch_r)\n",
    "        \n",
    "#         h = h.unsqueeze(1)    # unsqueeze 函数的作用是在第二维增加一个维度 1，h 的形状由 batch size * dim 变为 batch size * 1 * dim\n",
    "#         t = t.unsqueeze(1)\n",
    "#         r = r.unsqueeze(1)\n",
    "        \n",
    "#         conv_input = torch.cat([h, r, t], 1)       # 在第二个维度进行拼接，形状为 batch size * 3 * dim\n",
    "        \n",
    "#         # 使用 nn.Conv1d实现方式时，将下面两句 transpose 和 unsqueeze 的操作注释掉\n",
    "#         conv_input = conv_input.transpose(1, 2)    # 将第二个维度和第三个维度进行交换，形状变为 batch size * dim * 3\n",
    "#         conv_input = conv_input.unsqueeze(1)       # 卷积层的输入需要是 4D， batch size * 1 * dim * 3\n",
    "#         # 这里总结一下 1D 和 2D 卷积的共性：输入都是 4D，第一个维度是 batch size，第二个维度是通道数（即有几个矩阵），第三和第四维度指示矩阵的形状\n",
    "        \n",
    "#         conv_input = self.bn1(conv_input)          # 卷积前的 batch normalization\n",
    "#         conv_output = self.conv_layer(conv_input)  # 卷积，输出形状为 batch size * out_channels(卷积核数量) * (dim-self.config.kernel_size+1) * 1\n",
    "#         conv_output = self.bn2(conv_output)        # 卷积后的 batch normalization，形状不变\n",
    "#         conv_output = self.nonlinear(conv_output)  # 过激活函数，形状不变\n",
    "#         # 忽略 batch size，就是拍扁（拼接）为一条线\n",
    "#         conv_output = conv_output.view(-1, self.config.out_channels * (self.config.dim-self.config.kernel_size+1))\n",
    "#         # 卷积输出形状为二维平面，第一个维度是 batch size，第二维度是 feature map 的长条\n",
    "#         # batch size * (self.config.out_channels * (self.config.hidden_size-self.config.kernel_size+1))\n",
    "        \n",
    "#         fc_input = self.dropout(conv_output)\n",
    "#         fc_output = self.fc_layer(conv_output)    # batch size * 1\n",
    "#         score = fc_output.view(-1)             # batch size\n",
    "#         # print(score)\n",
    "#         return -score\n",
    "        #==============================================================================================================================\n",
    "        \n",
    "        #=====================================================ConvKB 1D 卷积===========================================================\n",
    "        # 现象记录：去掉 bn1 和 bn2 以及 fc_layer 前的 dropout，loss 就会卡在 69 下不去，但验证集上的正确率接近于 0\n",
    "        # 说明发生了过拟合，说明 batch norm 和 dropout 还是很有必要的\n",
    "        # 只去掉 batch norm，保留 dropout，也发生了同样的情况\n",
    "        # 说明 batch norm 非常有必要\n",
    "        # 实践证明，不进行 batch norm 是 loss 卡在 69 下不去的决定因素，dropout 的作用其次；卷积前后的 batch norm 都很有必要\n",
    "        \n",
    "        # 注意，这里和 Conv3D 遇到的情况还不太一样，这里的 l_filter 和 r_filter 都是 0，所以平均值也是0\n",
    "        # Conv3D 的情况是 r_filter 是 1，是比较有意思的现象\n",
    "        \n",
    "#         # 查表得到 embeddings\n",
    "#         h = self.ent_embeddings(self.batch_h)\n",
    "#         t = self.ent_embeddings(self.batch_t)\n",
    "#         r = self.rel_embeddings(self.batch_r)\n",
    "        \n",
    "#         h = h.unsqueeze(1)    # unsqueeze 函数的作用是在第二维增加一个维度 1，h 的形状由 batch size * dim 变为 batch size * 1 * dim\n",
    "#         t = t.unsqueeze(1)\n",
    "#         r = r.unsqueeze(1)\n",
    "        \n",
    "#         conv_input = torch.cat([h, r, t], 1)       # 在第二个维度进行拼接，形状为 batch size * 3 * dim\n",
    "        \n",
    "#         # 使用 nn.Conv1d实现方式时，将下面两句 transpose 和 unsqueeze 的操作注释掉\n",
    "#         conv_input = conv_input.transpose(1, 2)    # 将第二个维度和第三个维度进行交换，形状变为 batch size * dim * 3\n",
    "#         conv_input = conv_input.unsqueeze(1)       # 卷积层的输入需要是 4D， batch size * 1 * dim * 3\n",
    "#         # 这里总结一下 1D 和 2D 卷积的共性：输入都是 4D，第一个维度是 batch size，第二个维度是通道数（即有几个矩阵），第三和第四维度指示矩阵的形状\n",
    "        \n",
    "#         conv_input = self.bn1(conv_input)          # 卷积前的 batch normalization\n",
    "#         conv_output = self.conv_layer(conv_input)  # 卷积，输出形状为 batch size * out_channels(卷积核数量) * (dim-self.config.kernel_size+1) * 1\n",
    "#         conv_output = self.bn2(conv_output)        # 卷积后的 batch normalization，形状不变\n",
    "#         conv_output = self.nonlinear(conv_output)  # 过激活函数，形状不变\n",
    "#         # 忽略 batch size，就是拍扁（拼接）为一条线\n",
    "#         conv_output = conv_output.view(-1, self.config.out_channels * (self.config.dim-self.config.kernel_size+1))\n",
    "#         # 卷积输出形状为二维平面，第一个维度是 batch size，第二维度是 feature map 的长条\n",
    "#         # batch size * (self.config.out_channels * (self.config.hidden_size-self.config.kernel_size+1))\n",
    "        \n",
    "#         fc_input = self.dropout(conv_output)\n",
    "#         fc_output = self.fc_layer(fc_input)    # batch size * 1\n",
    "#         score = fc_output.view(-1)             # batch size\n",
    "#         # print(score)\n",
    "#         return -score\n",
    "        #==============================================================================================================================\n",
    "        \n",
    "\n",
    "        #==============================================ConvE 2D 图片卷积===========================================================\n",
    "#         # 问题：验证集上的正确率很低，测试集上跑出了点东西，正确率和验证集一样低\n",
    "#         # 猜想：在训练集上过拟合了\n",
    "        \n",
    "#         # 问题：过滤器个数由 64 改为 16 以及将 drop_prob 同样改为 0.5 之后，同样出现了 loss 到 69 死活下不去的情况\n",
    "#         # 原因：可能是过滤器个数由 64 改为 16 引起的\n",
    "        \n",
    "#         # 将 dim1 和 dim2 分别设置为 1 和 30，正确率立马就有了\n",
    "#         # 所以，可能是二维卷积没有用？\n",
    "#         # 二维图片拼接可能没什么用，直接调试三维吧\n",
    "        \n",
    "#         # 输入的 h 和 r 进行 reshape\n",
    "#         reshaped_h = self.ent_embeddings(self.batch_h).view(-1, 1, self.config.dim1, self.config.dim2)    # batch size * 1 * self.dim1 * self.dim2\n",
    "#         reshaped_r = self.rel_embeddings(self.batch_r).view(-1, 1, self.config.dim1, self.config.dim2)\n",
    "#         reshaped_t = self.ent_embeddings(self.batch_t).view(-1, 1, self.config.dim1, self.config.dim2)\n",
    "#         # print(reshaped_h.shape)\n",
    "        \n",
    "#         # reshpe 后对 h 和 r 两张图片进行堆叠\n",
    "#         # 在第二个维进行拼接，维度为 batch size * 1 * self.dim1×3 * self.dim2\n",
    "#         stack_input = torch.cat([reshaped_h, reshaped_r, reshaped_t], 2)    \n",
    "#         # print(stack_input.shape)\n",
    "        \n",
    "#         # 过 ConvE 网络\n",
    "#         x = self.bn1(stack_input)\n",
    "#         # x = self.input_dropout(x)    # batch size * 1 * self.dim1×3 * self.dim2\n",
    "#         x = self.conv_layer(x)    # batch size * out_channels * a * b\n",
    "#         # print(x.shape)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.nonlinear(x)\n",
    "#         x = self.feature_map_dropout(x)\n",
    "#         # print(x.shape)\n",
    "#         x = x.view(x.shape[0], -1)    # 保留 batch size 维度，拍扁为一个一维向量\n",
    "#         # print(x.shape)\n",
    "#         x = self.fc_layer(x)\n",
    "#         # print(x.shape)                # batch size * 1\n",
    "#         # x = self.hidden_layer_dropout(x)\n",
    "#         # x = self.bn2(x)\n",
    "#         # x = F.relu(x)\n",
    "#         score = x.view(-1)             # batch size\n",
    "#         return -score\n",
    "        #==============================================================================================================================\n",
    "        \n",
    "        #==============================================Conv3D My Model 视频卷积=========================================================\n",
    "        # 想法：能不能经过多层卷积最后卷成一个得分\n",
    "        # 回答：单层卷积即会导致过拟合，多层卷积过拟合可能会更严重\n",
    "        \n",
    "        # 问题：模型 loss 到 69 之后死活下不去，并且在验证集上正确率 0.5，训练集上无结果？\n",
    "        # 透过现象看本质，看 validation C++ 源码探究为什么验证集上的准确率会是 0.5\n",
    "        # 0.5 是 l_filter（0） 和 r_filter（1）的平均，说明网络没有学到东西\n",
    "        \n",
    "        # 猜想：69应该是这个网络趋于稳定的一个值，可能在训练集上过拟合了，所以验证集上的正确率上不去\n",
    "        # PlanB：设计多层卷积，最后卷成一个得分\n",
    "        # 回答：本来效果就很差，用两层卷积越训效果越差，应该是过拟合了\n",
    "        # 出现的现象是：loss 和 accuracy 都下降，双降表示模型在训练数据中一头扎得太深\n",
    "        \n",
    "        # 三维卷积的网络结构似乎对于三元组数据集来说过于复杂，因此验证集上的准确率不但上不去，反而随着训练下降\n",
    "        # 应该测试 Conv3D 取特值时即 ConvKB 时的效果\n",
    "        # 尝试用 convKB 的方式实现三维卷积？\n",
    "        # 新心得：loss其实不重要，不必过于纠结 loss 不下降，因为会有这样的现象：loss稳定了，但是随着训练，验证集上的正确率还会上升\n",
    "        # 只专注 acc debug 或调整网络结构即可，不必太过纠结中间量 loss\n",
    "        \n",
    "        # batch 的 h、r、t 查表得到 embeddings\n",
    "        h = self.ent_embeddings(self.batch_h)\n",
    "        r = self.rel_embeddings(self.batch_r)\n",
    "        t = self.ent_embeddings(self.batch_t)\n",
    "        \n",
    "        # 进行 reshape\n",
    "        h = h.view(-1, 1, self.config.dim1, self.config.dim2)    # batch size * 1 * dim1 * dim2\n",
    "        r = r.view(-1, 1, self.config.dim1, self.config.dim2)\n",
    "        t = t.view(-1, 1, self.config.dim1, self.config.dim2)    # 跪了跪了跪了跪了，磕头一万次，没想到 loss死活下不去的原因竟然是因为 copy paste\n",
    "                                                                 # r 和 t 都用 h 的值来 reshape，loss 能下去才怪！！！\n",
    "\n",
    "        cube_hrt = torch.cat([h,r,t], 1)    # batch size * 3 * dim1 * dim2，叠成了立方体\n",
    "        x = cube_hrt.unsqueeze(1)        # 在第二个维度增加维度为 1 的维度（每帧图片的通道数为 1），因为 Conv3d 的输入需要 5 维\n",
    "        x = self.bn1(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.nonlinear(x)\n",
    "        \n",
    "        # 这里应该有个缓冲？\n",
    "        # 除去 batch size，拍扁为一维向量，第二维是 out_channels × 卷积后的立方体大小\n",
    "        x = x.view(-1, self.conv_size)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_layer(x)\n",
    "        score = x.view(-1)\n",
    "        # print(score)\n",
    "        \n",
    "        return -score\n",
    "        #========================================================================================================================\n",
    "    \n",
    "    \n",
    "    def forward(self):\n",
    "        '''\n",
    "        前向传播计算 loss，返回该 batch 的 loss\n",
    "        '''\n",
    "        batch_score = self.cal_score()\n",
    "        \n",
    "        h = self.ent_embeddings(self.batch_h)\n",
    "        r = self.rel_embeddings(self.batch_r)\n",
    "        t = self.ent_embeddings(self.batch_t)\n",
    "        # 跪了orz，我说为什么正确率上不去，逐行对代码才发现又抄错了，r 的查表也写成了 ent_embeddings\n",
    "        # 改过来之后 loss 就下降得很快了\n",
    "        \n",
    "        l2_regular = torch.mean(h ** 2) + torch.mean(t ** 2) + torch.mean(r ** 2)\n",
    "        for p in self.conv_layer.parameters():\n",
    "            l2_regular += p.norm(2)\n",
    "        for p in self.fc_layer.parameters():\n",
    "            l2_regular += p.norm(2)\n",
    "            \n",
    "        # 该 batch 的 loss：该 batch 中所有样本得分的平均值 + 正则项\n",
    "        mean = torch.mean(self.criterion(self.batch_y * batch_score))    # 三维卷积的话，这部分降到 0.69 就死活下不去了\n",
    "        regular = self.config.lmbda * l2_regular\n",
    "        # print(mean.data, regular)\n",
    "        loss = mean + regular\n",
    "        # print(loss.data)\n",
    "        return loss\n",
    "\n",
    "conv3d = Conv3D(con)\n",
    "conv3d.batch_h = torch.LongTensor([1,40,5,4,6,79])\n",
    "conv3d.batch_r = torch.LongTensor([5,4,2,4,1,9])\n",
    "conv3d.batch_t = torch.LongTensor([3,60,80,3,56,7])\n",
    "conv3d.batch_y = torch.LongTensor([1,1,1,-1,-1,-1])\n",
    "conv3d()\n",
    "print(conv3d.rel_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    \n",
    "    def __init__(self, config, model):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.clib = self.config.clib\n",
    "        \n",
    "    def set_model(self, mode = 'train'):\n",
    "        self.model.to(device)\n",
    "        \n",
    "        if mode == 'train':    # 训练模型\n",
    "            print(\"Initializing training model...\")\n",
    "            # 为训练模型设定优化器\n",
    "            if self.config.opt_method == \"Adagrad\":\n",
    "                self.optimizer = optim.Adagrad(\n",
    "                    params = self.model.parameters(),\n",
    "                    lr = self.config.learning_rate,\n",
    "                    lr_decay = 0,\n",
    "                    weight_decay = 0\n",
    "                )\n",
    "            elif self.config.opt_method == \"Adadelta\":\n",
    "                self.optimizer = optim.Adadelta(\n",
    "                    params = self.model.parameters(),\n",
    "                    lr = self.config.learning_rate,\n",
    "                    weight_decay = 0\n",
    "                )\n",
    "            elif self.config.opt_method == \"Adam\":\n",
    "                self.optimizer = optim.Adam(\n",
    "                    params = self.model.parameters(),\n",
    "                    lr = self.config.learning_rate,\n",
    "                    weight_decay = 0\n",
    "                )\n",
    "            else:    # 不是以上三种的话就用 SGD\n",
    "                self.optimizer = optim.SGD(\n",
    "                    params = self.model.parameters(),\n",
    "                    lr = self.config.learning_rate,\n",
    "                    weight_decay = 0\n",
    "                )\n",
    "            print(\"Training model has been initialized.\")\n",
    "        else:                           # mode == 'test'，从 checkpoints 中载入模型，用于测试\n",
    "            print(\"Fetching model for test...\")\n",
    "            ckpt_path = os.path.join(\"./checkpoints/\", self.config.dataset + \"-netparam_best\" + \".ckpt\")  \n",
    "            self.model.load_state_dict(torch.load(ckpt_path))\n",
    "            self.model.to(device)\n",
    "            self.model.eval()\n",
    "            print(\"Test model has been loaded.\")\n",
    "    \n",
    "    def get_parameters(self, param_dict, mode = 'numpy'):\n",
    "        '''\n",
    "        从 model 中剥离出参数\n",
    "        '''\n",
    "        res = dict()\n",
    "        for param in param_dict:\n",
    "            if mode == 'numpy':\n",
    "                res[param] = param_dict[param].cpu().numpy()\n",
    "            elif mode == 'list':\n",
    "                res[param] = param_dict[param].cpu().numpy().tolist()\n",
    "            else:\n",
    "                res[param] = param_dict[param]\n",
    "        return res\n",
    "    \n",
    "    def neg_sample(self):\n",
    "        '''\n",
    "        对 batch 数据进行负采样\n",
    "        无返回值\n",
    "        '''\n",
    "        self.negative_ent = 1    # 负样本实体一个\n",
    "        self.negative_rel = 0\n",
    "        self.batch_seq_size = self.config.batch_size * (1 + self.negative_ent + self.negative_rel)\n",
    "        \n",
    "        self.batch_h = np.zeros(self.batch_seq_size, dtype = np.int64)    # 容量是 batch size 的两倍，用于盛放负样本\n",
    "        self.batch_t = np.zeros(self.batch_seq_size, dtype = np.int64)\n",
    "        self.batch_r = np.zeros(self.batch_seq_size, dtype = np.int64)\n",
    "        self.batch_y = np.zeros(self.batch_seq_size, dtype = np.float32)\n",
    "\n",
    "        self.batch_h_addr = self.batch_h.__array_interface__[\"data\"][0]\n",
    "        self.batch_t_addr = self.batch_t.__array_interface__[\"data\"][0]\n",
    "        self.batch_r_addr = self.batch_r.__array_interface__[\"data\"][0]\n",
    "        self.batch_y_addr = self.batch_y.__array_interface__[\"data\"][0]\n",
    "        \n",
    "        # 这一步将数据集中实体和关系的 id 传进来\n",
    "        # print(self.batch_y)\n",
    "        self.clib.sampling(\n",
    "            self.batch_h_addr,    # 头实体 batch 的地址，传给 clib 函数的指针\n",
    "            self.batch_t_addr,\n",
    "            self.batch_r_addr,\n",
    "            self.batch_y_addr,\n",
    "            self.config.batch_size,\n",
    "            self.negative_ent,\n",
    "            self.negative_rel\n",
    "        )\n",
    "    \n",
    "    def train_batch(self):\n",
    "        '''\n",
    "        使用 self.model 训练一个 batch 的数据\n",
    "        return: 该 batch 的 loss\n",
    "        '''\n",
    "        self.model.train()\n",
    "        # 向模型喂一个 batch 的数据\n",
    "        self.model.batch_h = torch.from_numpy(self.batch_h).to(device)    # numpy 数组转为 Tensor\n",
    "        self.model.batch_t = torch.from_numpy(self.batch_t).to(device)\n",
    "        self.model.batch_r = torch.from_numpy(self.batch_r).to(device)\n",
    "        self.model.batch_y = torch.from_numpy(self.batch_y).to(device)\n",
    "        # print(self.model.batch_y)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.model()    # 会自动调用 forward() 函数\n",
    "        loss.backward()        # 误差反向传播\n",
    "        # 由于在反向传播的过程中会发生梯度消失/爆炸，因此设定阈值，当梯度大于/小于阈值时候，将梯度缩放为阈值\n",
    "        nn.utils.clip_grad_norm_(parameters = self.model.parameters(), max_norm = 0.5, norm_type = 2)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def test_batch(self, model, batch_h, batch_t, batch_r):\n",
    "        '''\n",
    "        测试一个 batch 的数据\n",
    "        batch_h: numpy array\n",
    "        batch_t: numpy array\n",
    "        batch_r: numpy array\n",
    "        return: 该 test batch 的三元组得分\n",
    "        '''\n",
    "        # model.train()  将模块设置为训练模式，使用BatchNormalizetion()和Dropout()\n",
    "        # model.eval()   将模块设置为评估模式，不使用BatchNormalization()和Dropout()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            model.batch_h = torch.from_numpy(batch_h).to(device)\n",
    "            model.batch_t = torch.from_numpy(batch_t).to(device)\n",
    "            model.batch_r = torch.from_numpy(batch_r).to(device)\n",
    "        # print(\"test batch res is:\")\n",
    "        res = model.cal_score().cpu().data.numpy()\n",
    "        # print(res.shape)\n",
    "        return res\n",
    "        \n",
    "    def validation(self, model):\n",
    "        '''\n",
    "        验证模型\n",
    "        '''\n",
    "        model.eval()\n",
    "        self.vali_h = np.zeros(self.config.ent_num, dtype=np.int64)\n",
    "        self.vali_t = np.zeros(self.config.ent_num, dtype=np.int64)\n",
    "        self.vali_r = np.zeros(self.config.ent_num, dtype=np.int64)\n",
    "        self.vali_h_addr = self.vali_h.__array_interface__[\"data\"][0]    # array 的内存地址\n",
    "        self.vali_t_addr = self.vali_t.__array_interface__[\"data\"][0]\n",
    "        self.vali_r_addr = self.vali_r.__array_interface__[\"data\"][0]\n",
    "        \n",
    "        self.clib.validInit()\n",
    "        self.clib.getValidHit10.restype = ctypes.c_float\n",
    "        \n",
    "        print(\"The total number of validation triplets is %d\" % self.config.vali_num)\n",
    "        for i in range(self.config.vali_num):\n",
    "            sys.stdout.write(\"%d \\r\" % i)    # 动态打印输出\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # 之前运行这一步服务就会\n",
    "            # 原因：self.vali_r = np.zeros(self.config.ent_num, dtype=np.int64)，写成了rel_num，以为是原代码错了，但其实没有，自作聪明的结果\n",
    "            self.clib.getValidHeadBatch(self.vali_h_addr, self.vali_t_addr, self.vali_r_addr)\n",
    "            res = self.test_batch(model, self.vali_h, self.vali_t, self.vali_r)\n",
    "            self.clib.validHead(res.__array_interface__[\"data\"][0])\n",
    "            \n",
    "            self.clib.getValidTailBatch(self.vali_h_addr, self.vali_t_addr, self.vali_r_addr)\n",
    "            res = self.test_batch(model, self.vali_h, self.vali_t, self.vali_r)\n",
    "            self.clib.validTail(res.__array_interface__[\"data\"][0])\n",
    "            \n",
    "            # 第一个 batch 的 Hits@10 res 是 0.0\n",
    "        return self.clib.getValidHit10()   # 训练时的验证步骤，需要返回 hits@10 结果\n",
    "    \n",
    "    def test(self, model):\n",
    "        self.set_model(mode = 'test')\n",
    "        # 只做链接预测实验\n",
    "        \n",
    "        print(\"The total number of test triplets is %d\" % self.config.test_num)\n",
    "\n",
    "        self.test_h = np.zeros(self.config.ent_num, dtype=np.int64)\n",
    "        self.test_t = np.zeros(self.config.ent_num, dtype=np.int64)\n",
    "        self.test_r = np.zeros(self.config.ent_num, dtype=np.int64)\n",
    "        self.test_h_addr = self.test_h.__array_interface__[\"data\"][0]\n",
    "        self.test_t_addr = self.test_t.__array_interface__[\"data\"][0]\n",
    "        self.test_r_addr = self.test_r.__array_interface__[\"data\"][0]\n",
    "        \n",
    "        print(\"Testing...\")\n",
    "        for i in range(self.config.test_num):\n",
    "            sys.stdout.write(\"%d \\r\" % i)    # 动态打印输出\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            self.clib.getHeadBatch(self.test_h_addr, self.test_t_addr, self.test_r_addr)\n",
    "            res = self.test_batch(model, self.test_h, self.test_t, self.test_r)\n",
    "            self.clib.testHead(res.__array_interface__[\"data\"][0])\n",
    "\n",
    "            self.clib.getTailBatch(self.test_h_addr, self.test_t_addr, self.test_r_addr)\n",
    "            res = self.test_batch(model, self.test_h, self.test_t, self.test_r)\n",
    "            self.clib.testTail(res.__array_interface__[\"data\"][0])\n",
    "\n",
    "        self.clib.test_link_prediction()\n",
    "        print(\"Finished testing.\")\n",
    "    \n",
    "    def train_model(self):\n",
    "        if not os.path.exists(self.config.checkpoint_path):\n",
    "            os.mkdir(self.config.checkpoint_path)\n",
    "            \n",
    "        self.set_model(mode = self.config.mode)\n",
    "        \n",
    "        best_epoch = 0\n",
    "        best_hits10 = 0.0\n",
    "        best_model = self.model\n",
    "\n",
    "        epochs = tqdm(range(self.config.epoch_num))\n",
    "        \n",
    "        for epoch in epochs:\n",
    "            res = 0.0    # 用于累加本 epoch 各个 batch 的 loss\n",
    "            for batch in range(self.config.batch_num):    # 训练一个batch\n",
    "                self.neg_sample()    # 负采样\n",
    "                loss = self.train_batch()    # 训练一个 batch 为一个 step\n",
    "                # print(\"batch loss: %f\" % loss)\n",
    "                res += loss\n",
    "                \n",
    "            epochs.set_description(\"Epoch %d | loss: %f\" % (epoch, res))    # 输出进度条的描述\n",
    "            # 问题：loss到了69左右就不下降了，但是 50 epoch 后验证集正确率还是达到了 50多（但不再增长），所以应该问题不大\n",
    "            # 应该是网络比较复杂，在数据集上很快就拟合好了\n",
    "            # 验证集准确率上升很快，测试集上却没有跑出结果，应该是过拟合了\n",
    "            # 下一步是学习 ConvE 疯狂 dropout\n",
    "            \n",
    "            if (epoch + 1) % self.config.save_epoch == 0:\n",
    "                epochs.set_description(\"Epoch %d has finished, loss is %f, saving checkpoint ...\" % (epoch, res))\n",
    "                # 存储 checkpoint\n",
    "                save_path = os.path.join(self.config.checkpoint_path, self.config.dataset + \"-\" + str(epoch) + \".ckpt\")\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "            \n",
    "            if (epoch + 1) % self.config.vali_epoch == 0:\n",
    "                epochs.set_description(\"Epoch %d has finished, loss is %f, validating ...\" % (epoch, res))\n",
    "                hits10 = self.validation(self.model)\n",
    "                print(\"hits@10 of this validation epoch is: %.8f\" % hits10)\n",
    "#                 print(\"Testing on test set ...\")\n",
    "#                 self.test(self.model)\n",
    "#                 print(\"Test result is printed on Linux shell.\")\n",
    "                \n",
    "                if hits10 > best_hits10:\n",
    "                    best_hits10 = hits10\n",
    "                    best_epoch = epoch\n",
    "                    best_model = self.model\n",
    "                    \n",
    "            # sys.exit()\n",
    "            \n",
    "        # 所有的 epoch 都循环完之后（300个），存储验证集上最优模型的网络参数和 embeddings\n",
    "        print(\"Best epoch is %d, best hit@10 of validation set is %f\" % (best_epoch, best_hits10))\n",
    "        print(\"Storing checkpoint of best result at epoch %d ...\" % (best_epoch))\n",
    "        netparam_save_path = os.path.join(self.config.checkpoint_path, self.config.dataset + \"-netparam_best\" + \".ckpt\")\n",
    "        torch.save(best_model.state_dict(), netparam_save_path)\n",
    "\n",
    "        embed_save_path = os.path.join(self.config.checkpoint_path, self.config.dataset + \"-embed_best\" + \".json\")\n",
    "        with open(embed_save_path, 'w') as f:\n",
    "            f.write(json.dumps(self.get_parameters(best_model.state_dict(), 'list')))\n",
    "        print(\"Finished Storing best model and embeddings.\")\n",
    "\n",
    "        self.test(model = best_model)    # 测试结果会输出在 Linux 终端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(con, conv3d)\n",
    "if runner.config.mode == 'train':\n",
    "    runner.train_model()\n",
    "else:\n",
    "    runner.test(runner.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv3D 草稿\n",
    "# 假设 dim = 30，dim = 5，dim = 6\n",
    "# 输入图片的通道数是 1，输出通道/需要的卷积核数量是 16，kernel 每次处理 2 帧图片，卷积核大小为 3*4\n",
    "conv3d = nn.Conv3d(in_channels=1, out_channels=16, kernel_size=(2,3,4))\n",
    "# 输入是 5 维的，batch size 为 5，每帧图片通道数为 1，一段视频包含 3 帧图片（h,r,t），每帧图片大小为 5*6\n",
    "input = torch.randn(5, 1, 3, 5, 6)\n",
    "output = conv3d(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 造样本数据进行实验\n",
    "con = Config()\n",
    "conv3d = Conv3D(con)\n",
    "print(conv3d.ent_embeddings)\n",
    "print(conv3d.rel_embeddings)\n",
    "conv3d.batch_h = torch.LongTensor([9,2,5,6,8])    # batch size 为 5\n",
    "conv3d.batch_t = torch.LongTensor([6,7,3,4,6])\n",
    "conv3d.batch_r = torch.LongTensor([4,3,1,5,2])\n",
    "conv3d.cal_score()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
